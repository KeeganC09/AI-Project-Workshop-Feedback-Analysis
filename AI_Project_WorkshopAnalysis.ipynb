{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1aoajMzpU-g",
    "outputId": "cd58dc80-5acc-4625-dd33-e505a96c7f23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "from statistics import mode\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#from google.colab import files\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dOhrivuuUr-u"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reading the Dataset\n",
    "orig_data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Initializing the list for our bag of words\n",
    "all_words =  []\n",
    "\n",
    "# In POS tagging, adjectives denoted by \"J\", thus we specify it as an allowed word type\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "# Initializing a list for the IMDB Dataset\n",
    "documents = []\n",
    "\n",
    "# Fetching stop words from NLTK library and removing the word \"not\" from the list of stop words\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.remove('not')\n",
    "\n",
    "\n",
    "for p,q in zip(orig_data['review'],orig_data['sentiment']):\n",
    "  # Appending the dataset to the list \n",
    "  documents.append((p,q))\n",
    "\n",
    "  # Removing punctuations and other special characters, leaving behind only letters\n",
    "  cleaned = re.sub(r'[^(a-zA-Z)\\s]','',p)\n",
    "  \n",
    "  # Making all the words lowercase for homogenuity\n",
    "  lowered = cleaned.lower()\n",
    "\n",
    "  # Tokenizing the words to obtain list of words rather than full sentence\n",
    "  tokenized = word_tokenize(lowered)\n",
    "\n",
    "  # Removing any stopwords like \"and\" \"the\" \n",
    "  stopped = [w for w in tokenized if not w in stop_words]\n",
    "  \n",
    "  # POS tagging each word\n",
    "  pos = nltk.pos_tag(stopped)\n",
    "\n",
    "  for w in pos:\n",
    "    if w[1][0] in allowed_word_types:\n",
    "      # Checking if the word is adjective, only then appending it to the bag of words\n",
    "      all_words.append(w[0].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_DOD3zUf1S9",
    "outputId": "84d2aa81-2b3b-40bc-d818-cbf54079d08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains(bad)\n"
     ]
    }
   ],
   "source": [
    "# Making a Frequency Distribution to sort adjectives by their occurence\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# Keeping the 1000 most occuring adjectives\n",
    "word_features = list(all_words.keys())[:1000]\n",
    "\n",
    "# Grouping outliers/traps with our particular model in a dictionary\n",
    "risk_words = {\n",
    "    'not good' : 'bad',\n",
    "    'not bad' : 'good',\n",
    "    'not nice' : 'bad',\n",
    "    'hate' : 'bad'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Adding the list of risk_words to the word_feature list\n",
    "for i in risk_words:\n",
    "  word_features.append(i)\n",
    "\n",
    "# Defining a function to find all features in a given line based on the BagOfWords (making a one-hot-encoding)\n",
    "def find_features(word_features,document):\n",
    "    \"\"\"\n",
    "    In this function, suppose we give an input as \" The workshop was wonderful and interesting, but a little boring towards the end\"\n",
    "    The function will then return a 1004 size dictionary, where for each feature (adjective) in our BoW, we either have True, ie \n",
    "    the adjective is in the sentence, or False, meaning the adjective is not in the sentence\n",
    "\n",
    "    Thus, 'wonderful', 'interesting' and 'boring' features will have true while the rest have false.\n",
    "    \"\"\"\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    # Converting into set to remove duplicates to save time \n",
    "    document_words = set(words)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        # w in document_words will either return True or False\n",
    "        features['contains({})'.format(w)] = (w in document_words)\n",
    "    for w in risk_words.keys():\n",
    "        if w in document:\n",
    "          features['contains({})'.format(risk_words[w])] = (w in document)\n",
    "          if w != 'hate':\n",
    "            # For a sentence containing 'not good' we make the 'good' feature False to avoid the trap of thinking good occurs in the sentence\n",
    "            features['contains({})'.format(w.split()[1])] = False\n",
    "    return features\n",
    "\n",
    "def show_features(word_features,document):\n",
    "    \"\"\"\n",
    "    Same functionality as find_features() used to display the features for better understanding\n",
    "    \"\"\"\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    document_words = set(words)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features['contains({})'.format(w)] = (w in document_words)\n",
    "    for w in risk_words.keys():\n",
    "        if w in document:\n",
    "          features['contains({})'.format(risk_words[w])] = (w in document)\n",
    "          if w != 'hate':\n",
    "            features['contains({})'.format(w.split()[1])] = False\n",
    "    for w in features.keys():\n",
    "      if features[w] == True:\n",
    "        print(w)\n",
    "\n",
    "temp = show_features(word_features,'The movie was not good')\n",
    "\n",
    "# Making our training set as the features for every sentence in the IMDB Model with it's corresponding sentiment\n",
    "featuresets = [(find_features(word_features,review),category) for (review,category) in documents]\n",
    "\n",
    "# Shuffling it to prevent unnecessary biasing\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vzz5KGlFKFHx",
    "outputId": "f8edc681-ae41-4c9e-ae74-bcdfc5aca030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOOGiwu1kzDg",
    "outputId": "fc739490-9b3c-4de0-8da7-9db45654d19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 82.17\n"
     ]
    }
   ],
   "source": [
    "training_set = featuresets[:40000]\n",
    "testing_set = featuresets[40000:]\n",
    "\n",
    "# An inbuilt Naive Bayes Classifier is built to save time and effort on training \n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lwKtUs1wn1Dg",
    "outputId": "ef4acc56-399e-402a-cdbc-2a814da02cfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment(classifier,text):\n",
    "    feats = find_features(word_features,text)\n",
    "    return classifier.classify(feats)#, classifier.confidence(feats)\n",
    "\n",
    "sentiment(classifier,'The worksop was very amazing and very exciting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sentiment Inception.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
